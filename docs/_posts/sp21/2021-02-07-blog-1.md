---
layout: post
title: "Blog:0 Apache Spark Setup"
date: 2021-02-09 4:10:00 -0700
categories: Setting up Apache Spark 
---

# Setting up Apache Spark on Windows

![/assets/images/spring_21/blog0.png]
# Apache Spark

Apache Spark is a data processing framework that is used for processing very large data sets, and can also
distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools.

# Apache Spark Architecture

At a fundamental level, an Apache Spark application consists of two main components: 
**driver** whcih converts user's code into multiple tasks that can be distributed across worker nodes, 
and **executors** which run on those nodes and execute the tasks assigned to them. Some of the cluster manager 
(Standalone, Hadoop YARN, Kubernetes) are necessary to mediate between two.

# Local Mode

Spark can run in a **standalone** cluster mode that simply requires the Apache Spark framework and **JVM** on each 
machine your cluster. 

**Spark**, in addition to its cluster mode also has **Local** mode. The driver and executors are simply processes, which 
**live** on the same machine or different machines.

In Local mode, the driver and executor runs as **Threads** on your individual computer instead of a cluster.
 
