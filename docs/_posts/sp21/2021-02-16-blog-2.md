---
layout: post
title: "Blog: 1 PySpark Installation and setup"
date: 2021-02-16 10:47:00 -0700
categories: Installing Apache Spark
---

# Apache Spark and Pyspark

![pyspark](/assets/images/spring_21/blog_1/pyspark.png)

In the previous blog, I talked about installing the required binary files and Path setup to run Apache on the 
windows. Apache Spark is an analytics engine and parallel computation framework with **Scala, Python and R 
interfaces.**
The Spark framework can load the data directl from disk, memory and other data storage technologies such as 
Amazon S3, Hadoop Distributed File System (HDFS), HBase, and others.


# Setup Steps

As I have already discussed on setting up environment variables and Path on my previous blog. 

* SPARK_HOME
* JAVA_HOME
* HADOOP_HOME
* winutils.exe

[https://github.com/sagarlimbu0/sagarlimbu0.github.io/blob/master/docs/_posts/sp21/2021-02-07-blog-1.md]

In this blog, I will provide the steps for **PySpark** installation process using Anaconda on **Jupyter Notebook.**

> conda install -c conda-forge findspark

After performing the above steps, you can check import the module and check the pyspark on the jupyter notebook.

```
import findspark

findspark.find()

```
**output:**

```
'C:\\Users\\limbu\\spark-3.0.1-bin-hadoop3.2'
```



