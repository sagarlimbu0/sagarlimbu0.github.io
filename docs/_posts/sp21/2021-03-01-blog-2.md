---
layout: post
title: "Blog: 2 Introduction to PySpark"
date: 2021-03-01 10:47:00 -0700
categories: PySpark with Apache Spark
---
[python_spark](/assets/images/spring_21/blog_2/python_spark.png)

# Introduction to PySpark

PySpark is a Python API for Spark released by the Apache Spark community to support Python with spark. Apache Spark is a popular
open source framework that ensures data processing in a reliable speed and supports various languages like Scala, Python, Java, and R.

In this blog, I will demonstrate the basic understanding of PySpark using Jupyter Notebook. **PySpark** is the collaboration of Apache Spark
and Python. 

# Reason for Selecting Pyhton to work with SPARK

* Python provides simple and comprehensive API
* Python is a simple and easy to learn in comparison with other programming languages
* Python is backed by huge and active community
* Python has an abundant **Libraries** to perform data visualization and data manipulations like **NumPy, Scikit-learn, seaborn, etc.**

# Spark Session

Spark session is an unified **entry point** of a SPARK application that allows to interact with various **spark's** functionality.
If someone is familiar with terms like **spark context, hive context, SQL context**, now all of these context is **enscapulated** with
spark **session**.

Following lines of code shows the steps for creating a spark **session**.

```
import findspark

findspark.find()
```
**output:**

> 'C:\\Users\\limbu\\spark-3.0.1-bin-hadoop3.2'

This determines the installed location for Pyspark directory.

Now, we can create a SPARK session using the jupyter notebook.

```
from pyspakr.sql.functions import SparkSession

spark= SparkSession\
.builder\
.getOrCreate()\
.appName("findInfo")
```


