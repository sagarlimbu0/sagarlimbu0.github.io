---
layout: post
title: "Blog: 1 PySpark Installation and setup"
date: 2021-02-16 10:47:00 -0700
categories: Installing Apache Spark
---

# Apache Spark and Pyspark

![pyspark](/assets/images/spring_21/blog_1/pyspark.png)

In the previous blog, I talked about installing the required binary files and Path setup to run Apache on the 
windows. Apache Spark is an analytics engine and parallel computation framework with **Scala, Python and R 
interfaces.**

The Spark framework can load the data directl from disk, memory and other data storage technologies such as 
Amazon S3, Hadoop Distributed File System (HDFS), HBase, and others.


# Setup Steps

As I have already discussed on setting up environment variables and Path on my previous blog. 

* SPARK_HOME
* JAVA_HOME
* HADOOP_HOME
* winutils.exe

[blog_0](https://github.com/sagarlimbu0/sagarlimbu0.github.io/blob/master/docs/_posts/sp21/2021-02-07-blog-1.md)

In this blog, I will provide the steps for **PySpark** installation process using Anaconda on **Jupyter Notebook.**

> conda install -c conda-forge findspark

After performing the above steps, you can check import the module and check the pyspark on the jupyter notebook.

```
import findspark

findspark.find()

```
**output:**

```
'C:\\Users\\limbu\\spark-3.0.1-bin-hadoop3.2'
```

# Determing the PATH variables

Before approaching with the **PySpark** installation, we need to make sure the path variables are correctly initialized.
Since, the Apache Framework with Python is **fast** evolving infrastructure, it requires several necessary steps.

We can run the following commands to check the proper PATH setup:

```
import os

print(os.environ['SPARK_HOME'])
print(os.environ['JAVA_HOME'])
print(os.environ['HADOOP_HOME']) ```

**output:**

```
C:\Users\daiko\spark-3.0.1-bin-hadoop3.2
C:\Program Files\Java\jdk1.8.0_281
C:\Users\daiko\spark-3.0.1-bin-hadoop3.2```




